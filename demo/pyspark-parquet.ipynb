{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7431993",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a798aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('data/transactions_train.csv'):\n",
    "    ! mkdir -p data\n",
    "    ! curl -OL https://storage.yandexcloud.net/di-datasets/age-prediction-nti-sbebank-2019.zip\n",
    "    ! unzip -j -o age-prediction-nti-sbebank-2019.zip 'data/*.csv' -d data\n",
    "    ! mv age-prediction-nti-sbebank-2019.zip data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf847b53",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587df1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f989bc",
   "metadata": {},
   "source": [
    "## Pyspark Data Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1608b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/23 11:37:26 WARN Utils: Your hostname, vm2 resolves to a loopback address: 127.0.1.1; using 192.168.0.6 instead (on interface ens192)\n",
      "22/06/23 11:37:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/23 11:37:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/23 11:37:26 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', '192.168.0.6'),\n",
       " ('spark.driver.memoryOverhead', '4g'),\n",
       " ('spark.app.id', 'local-1655984247654'),\n",
       " ('spark.driver.port', '44961'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/kireev/pycharm-deploy/pytorch-lifestream/demo/spark-warehouse'),\n",
       " ('spark.local.dir', '../../spark_local_dir'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '16g'),\n",
       " ('spark.executor.memory', '16g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.startTime', '1655984246801'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PysparkDataPreprocessor'),\n",
       " ('spark.cores.max', '24'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.maxResultSize', '4g'),\n",
       " ('spark.executor.memoryOverhead', '4g')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "data_path = 'data/'\n",
    "\n",
    "spark_conf = pyspark.SparkConf()\n",
    "spark_conf.setMaster(\"local[*]\").setAppName(\"PysparkDataPreprocessor\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.cores.max\", \"24\")\n",
    "spark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27115f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_data = spark.read.options(header=True, inferSchema=True).csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c7afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.preprocessing import PysparkDataPreprocessor\n",
    "\n",
    "preprocessor = PysparkDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    cols_event_time='trans_date',\n",
    "    time_transformation='float',\n",
    "    cols_category=[\"trans_date\", \"small_group\"],\n",
    "    cols_log_norm=[\"amount_rur\"],\n",
    "    cols_identity=[],\n",
    "    print_dataset_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43a15ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/23 11:37:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/23 11:37:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/23 11:37:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/23 11:37:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/23 11:37:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/23 11:37:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/23 11:37:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/23 11:37:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/23 11:37:44 WARN CacheManager: Asked to cache already cached data.        \n",
      "[Stage 19:====================================================> (193 + 7) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 120 ms, total: 416 ms\n",
      "Wall time: 24.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_pysparkdf = preprocessor.fit_transform(source_data).persist()\n",
    "dataset_pysparkdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7582b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|client_id|          trans_date|         small_group|          amount_rur|          event_time|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      463|[726, 724, 725, 7...|[7, 1, 7, 3, 3, 2...|[0.32135927726085...|[1.0, 2.0, 5.0, 7...|\n",
      "|      471|[730, 726, 726, 7...|[68, 5, 4, 7, 1, ...|[0.26422890225828...|[0.0, 1.0, 1.0, 1...|\n",
      "|      496|[723, 723, 722, 7...|[3, 1, 1, 1, 1, 4...|[0.27119266729746...|[3.0, 3.0, 4.0, 5...|\n",
      "|      833|[726, 726, 726, 7...|[17, 15, 1, 49, 3...|[0.28922101008224...|[1.0, 1.0, 1.0, 2...|\n",
      "|     1238|[730, 723, 716, 7...|[3, 11, 3, 3, 1, ...|[0.13326234667635...|[0.0, 3.0, 9.0, 1...|\n",
      "|     1342|[730, 722, 702, 7...|[14, 3, 3, 21, 37...|[0.11547227625749...|[0.0, 4.0, 11.0, ...|\n",
      "|     1591|[724, 724, 723, 7...|[37, 20, 7, 6, 1,...|[0.35099852868535...|[2.0, 2.0, 3.0, 3...|\n",
      "|     1645|[724, 723, 723, 7...|[2, 16, 1, 1, 2, ...|[0.23653852058912...|[2.0, 3.0, 3.0, 4...|\n",
      "|     1829|[727, 716, 709, 7...|[3, 1, 3, 1, 1, 4...|[0.22955176609017...|[7.0, 9.0, 10.0, ...|\n",
      "|     1959|[723, 723, 722, 7...|[3, 1, 1, 1, 30, ...|[0.29358422166110...|[3.0, 3.0, 4.0, 5...|\n",
      "|     2122|[730, 722, 725, 7...|[38, 5, 14, 6, 3,...|[0.00341230998274...|[0.0, 4.0, 5.0, 5...|\n",
      "|     2142|[724, 724, 691, 6...|[7, 10, 1, 24, 11...|[0.29000320625500...|[2.0, 2.0, 13.0, ...|\n",
      "|     2659|[684, 688, 681, 6...|[3, 3, 36, 3, 1, ...|[0.25963118799171...|[27.0, 31.0, 39.0...|\n",
      "|     3175|[728, 728, 727, 7...|[4, 7, 1, 1, 2, 2...|[0.25272554086981...|[6.0, 6.0, 7.0, 1...|\n",
      "|     3749|[726, 722, 722, 7...|[3, 16, 35, 46, 1...|[0.23823234099543...|[1.0, 4.0, 4.0, 4...|\n",
      "|     3794|[723, 723, 722, 7...|[5, 10, 26, 26, 7...|[0.24852798380522...|[3.0, 3.0, 4.0, 7...|\n",
      "|     3997|[730, 726, 724, 7...|[3, 1, 29, 1, 1, ...|[0.35715492952800...|[0.0, 1.0, 2.0, 2...|\n",
      "|     4101|[723, 722, 725, 7...|[1, 1, 1, 13, 1, ...|[0.22094081440301...|[3.0, 4.0, 5.0, 6...|\n",
      "|     4818|[726, 724, 722, 7...|[1, 6, 6, 23, 6, ...|[0.26493123311313...|[1.0, 2.0, 4.0, 6...|\n",
      "|     4935|[723, 722, 722, 7...|[31, 31, 3, 10, 4...|[0.47558022334646...|[3.0, 4.0, 4.0, 4...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_pysparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35e3205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('client_id', 'int'),\n",
       " ('trans_date', 'array<int>'),\n",
       " ('small_group', 'array<int>'),\n",
       " ('amount_rur', 'array<double>'),\n",
       " ('event_time', 'array<float>')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pysparkdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3510707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test dataset: 5965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset 21646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of valid dataset 2389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "test_df = dataset_pysparkdf.sample(fraction=0.2)\n",
    "train_df = dataset_pysparkdf.subtract(test_df)\n",
    "\n",
    "valid_df = train_df.sample(fraction=0.1)\n",
    "train_df = train_df.subtract(valid_df)\n",
    "\n",
    "print('Size of test dataset:', test_df.count())\n",
    "print('Size of train dataset', train_df.count())\n",
    "print('Size of valid dataset', valid_df.count())\n",
    "\n",
    "test_df.write.parquet('test.parquet', mode='overwrite')\n",
    "train_df.write.parquet('train.parquet', mode='overwrite')\n",
    "valid_df.write.parquet('valid.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65b5e3",
   "metadata": {},
   "source": [
    "## Data access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "781deed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import ParquetDataset, ParquetFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a1c0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_train = ParquetDataset(ParquetFiles('train.parquet').data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e99e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': 6456,\n",
       " 'trans_date': tensor([724, 724, 723,  ...,  49,  49, 209], dtype=torch.int32),\n",
       " 'small_group': tensor([4, 1, 3,  ..., 4, 1, 2], dtype=torch.int32),\n",
       " 'amount_rur': tensor([0.1194, 0.3518, 0.2086,  ..., 0.2233, 0.1576, 0.1701],\n",
       "        dtype=torch.float64),\n",
       " 'event_time': tensor([  2.,   2.,   3.,  ..., 720., 720., 721.])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(iterable_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354c3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7cb807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\n",
    "\n",
    "map_processed_train = MemoryMapDataset(\n",
    "    data=iterable_train,\n",
    "    i_filters=[\n",
    "        SeqLenFilter(min_seq_len=25),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8086734c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': 6456,\n",
       " 'trans_date': tensor([724, 724, 723,  ...,  49,  49, 209], dtype=torch.int32),\n",
       " 'small_group': tensor([4, 1, 3,  ..., 4, 1, 2], dtype=torch.int32),\n",
       " 'amount_rur': tensor([0.1194, 0.3518, 0.2086,  ..., 0.2233, 0.1576, 0.1701],\n",
       "        dtype=torch.float64),\n",
       " 'event_time': tensor([  2.,   2.,   3.,  ..., 720., 720., 721.])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_processed_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ab1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da2f289b",
   "metadata": {},
   "source": [
    "**Attention!**\n",
    "\n",
    "You cannot use a pretrained `coles-emb.pt` with a different preprocessor than the one used to train the model.\n",
    "This is because preprocessor save specific category to embedding_id mapping during fit procedure.\n",
    "Model use this mapping during pretrain.\n",
    "Using different mapping at inference will corrupt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
